{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed22e108",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=r\"D:\\PROJECT_LABUBU\\ai-backend\\IPsec Notes.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09cc878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = []\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9b40ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc72fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "collection = chroma_client.get_or_create_collection(\"nsc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6787900f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Added 36 pages to Chroma\n"
     ]
    }
   ],
   "source": [
    "if collection.count() == 0:\n",
    "    docs = [page.page_content for page in pages]\n",
    "    ids = [f\"page_{i}\" for i in range(len(docs))]\n",
    "    embeddings = embedder.encode(docs).tolist()\n",
    "\n",
    "    collection.add(\n",
    "        documents=docs,\n",
    "        ids=ids,\n",
    "        embeddings=embeddings\n",
    "    )\n",
    "    print(f\" Added {len(docs)} pages to Chroma\")\n",
    "else:\n",
    "    print(\" Using existing Chroma collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2010d88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded!\n",
      "Database has 1 documents\n",
      "\n",
      "üéØ INTERACTIVE QUIZ TIME!\n",
      "==================================================\n",
      "Generating 5 questions from your documents...\n",
      "This may take a moment...\n",
      "Found 1 documents in database\n",
      "Retrieved content from your documents\n",
      "Generated question 1: [D]...\n",
      "Generated question 2: [D]...\n",
      "Generated question 3: [D]...\n",
      "Generated question 4: [A, B, C, or D]...\n",
      "Generated question 5: [D]...\n",
      "\n",
      "üìù Generated 5 questions! Let's begin:\n",
      "========================================\n",
      "\n",
      "Question 1: What type of information is primarily discussed in the text about the documents?\n",
      "  a) Modern scientific theories\n",
      "  b) Fictional stories and myths\n",
      "  c) Information about the documents from historical records\n",
      "  d) Contemporary political events\n",
      "‚úÖ Correct!\n",
      "\n",
      "Question 2: What type of information is primarily discussed in the text about the documents?\n",
      "  a) Contemporary political events\n",
      "  b) Information about the documents from historical records\n",
      "  c) Fictional stories and myths\n",
      "  d) Modern scientific theories\n",
      "‚úÖ Correct!\n",
      "\n",
      "Question 3: What type of information is primarily discussed in the text about the documents?\n",
      "  a) Modern scientific theories\n",
      "  b) Information about the documents from historical records\n",
      "  c) Fictional stories and myths\n",
      "  d) Contemporary political events\n",
      "Please enter a, b, c, or d\n",
      "Please enter a, b, c, or d\n",
      "‚ùå Wrong! Correct answer was B) Information about the documents from historical records\n",
      "\n",
      "Question 4: What type of information is primarily discussed in the text about the documents?\n",
      "  a) Modern scientific theories\n",
      "  b) Fictional stories and myths\n",
      "  c) Information about the documents from historical records\n",
      "  d) Contemporary political events\n",
      "‚ùå Wrong! Correct answer was C) Information about the documents from historical records\n",
      "\n",
      "Question 5: What type of information is primarily discussed in the text about the documents?\n",
      "  a) Contemporary political events\n",
      "  b) Modern scientific theories\n",
      "  c) Information about the documents from historical records\n",
      "  d) Fictional stories and myths\n",
      "‚ùå Wrong! Correct answer was C) Information about the documents from historical records\n",
      "\n",
      "==================================================\n",
      "üìä FINAL RESULTS\n",
      "==================================================\n",
      "Your Score: 2/5\n",
      "Percentage: 40.0%\n",
      "üìö Not bad! A bit more study will help!\n",
      "\n",
      "Thanks for taking the quiz on ! üéØ\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "import torch\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Load models\n",
    "print(\"Loading models...\")\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Connect to existing ChromaDB\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "collection = client.get_collection(\"history\")\n",
    "\n",
    "# FLAN-T5 setup\n",
    "model_id = \"google/flan-t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_id)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_id)\n",
    "\n",
    "print(\"Models loaded!\")\n",
    "print(f\"Database has {collection.count()} documents\")\n",
    "\n",
    "def generate_quiz_questions_from_docs(num_questions=5):\n",
    "    \"\"\"Generate quiz questions directly from all available documents\"\"\"\n",
    "    # Get all documents from the collection\n",
    "    all_results = collection.get()\n",
    "    \n",
    "    if not all_results or not all_results['documents']:\n",
    "        print(\"No documents found in the collection.\")\n",
    "        return None\n",
    "    \n",
    "    all_documents = all_results['documents']\n",
    "    print(f\"Found {len(all_documents)} documents in database\")\n",
    "    \n",
    "    # Combine multiple documents for richer context\n",
    "    combined_context = \"\\n\".join(all_documents)[:2500]\n",
    "    \n",
    "    print(\"Retrieved content from your documents\")\n",
    "    \n",
    "    # Simplified prompt that works better with FLAN-T5\n",
    "    prompt = f\"\"\"Create a multiple choice question based on this text:\n",
    "\n",
    "{combined_context}\n",
    "\n",
    "Question format:\n",
    "What [question about the content]?\n",
    "A. [option 1]\n",
    "B. [option 2] \n",
    "C. [option 3]\n",
    "D. [option 4]\n",
    "Correct answer: [A, B, C, or D]\n",
    "\n",
    "Create one question:\"\"\"\n",
    "    \n",
    "    questions = []\n",
    "    \n",
    "    # Generate questions one by one for better results\n",
    "    for i in range(num_questions):\n",
    "        # Use a different subset of documents for variety\n",
    "        start_idx = (i * len(all_documents) // num_questions) % len(all_documents)\n",
    "        doc_subset = all_documents[start_idx:start_idx + 3]  # Use 3 documents at a time\n",
    "        if len(doc_subset) < 3:\n",
    "            doc_subset = all_documents[:3]  # Fallback to first 3\n",
    "        \n",
    "        context = \"\\n\".join(doc_subset)[:2000]\n",
    "        \n",
    "        current_prompt = f\"\"\"Create a multiple choice question based on this text:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question format:\n",
    "What [question about the content]?\n",
    "A. [option 1]\n",
    "B. [option 2] \n",
    "C. [option 3]\n",
    "D. [option 4]\n",
    "Correct answer: [A, B, C, or D]\n",
    "\n",
    "Create one question:\"\"\"\n",
    "        \n",
    "        input_ids = tokenizer(current_prompt, return_tensors=\"pt\", max_length=512, truncation=True).input_ids\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=200,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                top_p=0.8,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # Remove the prompt from the response\n",
    "        response = response.replace(current_prompt, \"\").strip()\n",
    "        \n",
    "        print(f\"Generated question {i+1}: {response[:100]}...\")\n",
    "        \n",
    "        # Try to parse this single question\n",
    "        parsed_q = parse_single_question(response, \"the documents\", context)\n",
    "        if parsed_q:\n",
    "            questions.append(parsed_q)\n",
    "    \n",
    "    return questions\n",
    "\n",
    "def parse_single_question(response, topic, context):\n",
    "    \"\"\"Parse a single generated question\"\"\"\n",
    "    lines = [line.strip() for line in response.split('\\n') if line.strip()]\n",
    "    \n",
    "    question_text = None\n",
    "    options = []\n",
    "    correct_answer = None\n",
    "    \n",
    "    for line in lines:\n",
    "        # Look for question (usually starts with \"What\", \"Who\", \"When\", \"Where\", \"How\", \"Which\")\n",
    "        if any(line.startswith(word) for word in [\"What\", \"Who\", \"When\", \"Where\", \"How\", \"Which\", \"Why\"]) and \"?\" in line:\n",
    "            question_text = line\n",
    "        \n",
    "        # Look for options A, B, C, D\n",
    "        elif re.match(r'^[A-D][\\.\\)]\\s*', line, re.IGNORECASE):\n",
    "            options.append(line)\n",
    "        \n",
    "        # Look for correct answer\n",
    "        elif re.search(r'correct\\s*answer\\s*:?\\s*([A-D])', line, re.IGNORECASE):\n",
    "            match = re.search(r'correct\\s*answer\\s*:?\\s*([A-D])', line, re.IGNORECASE)\n",
    "            if match:\n",
    "                correct_answer = match.group(1).lower()\n",
    "    \n",
    "    # If parsing failed, create a simple question from context\n",
    "    if not question_text or len(options) < 4 or not correct_answer:\n",
    "        return create_fallback_question(topic, context)\n",
    "    \n",
    "    # Convert options to consistent format\n",
    "    formatted_options = []\n",
    "    for opt in options[:4]:  # Only take first 4 options\n",
    "        # Remove the letter prefix and add our own\n",
    "        clean_opt = re.sub(r'^[A-D][\\.\\)]\\s*', '', opt, flags=re.IGNORECASE)\n",
    "        formatted_options.append(clean_opt)\n",
    "    \n",
    "    if len(formatted_options) == 4:\n",
    "        return {\n",
    "            'question': question_text,\n",
    "            'options': formatted_options,\n",
    "            'correct_answer': correct_answer\n",
    "        }\n",
    "    \n",
    "    return create_fallback_question(topic, context)\n",
    "\n",
    "def create_fallback_question(topic, context):\n",
    "    \"\"\"Create a simple question when parsing fails\"\"\"\n",
    "    # Extract some key words from context\n",
    "    words = context.split()[:50]  # First 50 words\n",
    "    \n",
    "    # Create a simple question\n",
    "    correct_option = f\"Information about {topic} from historical records\"\n",
    "    wrong_options = [\n",
    "        \"Fictional stories and myths\",\n",
    "        \"Modern scientific theories\", \n",
    "        \"Contemporary political events\"\n",
    "    ]\n",
    "    \n",
    "    # Randomize option order\n",
    "    all_options = [correct_option] + wrong_options\n",
    "    random.shuffle(all_options)\n",
    "    correct_answer = chr(ord('a') + all_options.index(correct_option))\n",
    "    \n",
    "    return {\n",
    "        'question': f\"What type of information is primarily discussed in the text about {topic}?\",\n",
    "        'options': all_options,\n",
    "        'correct_answer': correct_answer\n",
    "    }\n",
    "\n",
    "# Main quiz function\n",
    "def run_interactive_quiz():\n",
    "    print(\"\\nüéØ INTERACTIVE QUIZ TIME!\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"Generating 5 questions from your documents...\")\n",
    "    print(\"This may take a moment...\")\n",
    "    \n",
    "    try:\n",
    "        questions = generate_quiz_questions_from_docs(5)\n",
    "        \n",
    "        if not questions or len(questions) == 0:\n",
    "            print(\"‚ùå Sorry, couldn't generate questions from the documents.\")\n",
    "            print(\"Make sure your ChromaDB has documents with sufficient content.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nüìù Generated {len(questions)} questions! Let's begin:\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        score = 0\n",
    "        \n",
    "        # Ask each question\n",
    "        for i, q in enumerate(questions, 1):\n",
    "            print(f\"\\nQuestion {i}: {q['question']}\")\n",
    "            \n",
    "            # Display options with a), b), c), d) format\n",
    "            option_letters = ['a', 'b', 'c', 'd']\n",
    "            for j, option in enumerate(q['options'][:4]):\n",
    "                print(f\"  {option_letters[j]}) {option}\")\n",
    "            \n",
    "            # Get user answer\n",
    "            while True:\n",
    "                answer = input(\"\\nYour answer (a/b/c/d): \").strip().lower()\n",
    "                if answer in ['a', 'b', 'c', 'd']:\n",
    "                    if answer == q['correct_answer']:\n",
    "                        print(\"‚úÖ Correct!\")\n",
    "                        score += 1\n",
    "                    else:\n",
    "                        correct_letter = q['correct_answer']\n",
    "                        correct_text = q['options'][ord(correct_letter) - ord('a')]\n",
    "                        print(f\"‚ùå Wrong! Correct answer was {correct_letter.upper()}) {correct_text}\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Please enter a, b, c, or d\")\n",
    "        \n",
    "        # Show final results\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"üìä FINAL RESULTS\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Your Score: {score}/{len(questions)}\")\n",
    "        percentage = (score / len(questions)) * 100\n",
    "        print(f\"Percentage: {percentage:.1f}%\")\n",
    "        \n",
    "        # Personalized feedback\n",
    "        if percentage == 100:\n",
    "            print(\"üéâ PERFECT SCORE! Outstanding knowledge!\")\n",
    "        elif percentage >= 80:\n",
    "            print(\"üåü Excellent work! You know your stuff!\")\n",
    "        elif percentage >= 60:\n",
    "            print(\"üëç Good job! Keep up the learning!\")\n",
    "        elif percentage >= 40:\n",
    "            print(\"üìö Not bad! A bit more study will help!\")\n",
    "        else:\n",
    "            print(\"üí™ Keep learning! Practice makes perfect!\")\n",
    "        \n",
    "        print(f\"\\nThanks for taking the quiz on ! üéØ\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during quiz: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run the quiz\n",
    "if __name__ == \"__main__\":\n",
    "    run_interactive_quiz()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
